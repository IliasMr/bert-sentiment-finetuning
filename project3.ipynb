{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c29d4c",
   "metadata": {},
   "source": [
    "## A. Data reading and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0cfaa0",
   "metadata": {},
   "source": [
    "*Notes:* In this notebook, I briefly mention the steps I followed for completing the project. A more detailed analysis exists on the respective report pdf.<br>\n",
    "I keep here all of the code I used during my experiments, event though it may not be used for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c872933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hlias/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re \n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from wordcloud import STOPWORDS\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import warnings\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import random\n",
    "# kaggle\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b649c",
   "metadata": {},
   "source": [
    "Random  seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59b24ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x766d963ffd90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 43 \n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b44d4",
   "metadata": {},
   "source": [
    "Reading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train_dataset.csv\")\n",
    "test_df = pd.read_csv(\"data/test_dataset.csv\")\n",
    "val_df = pd.read_csv(\"data/val_dataset.csv\")\n",
    "\n",
    "train_df.describe()\n",
    "\n",
    "# supress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "############ kaggle ############ \n",
    "# train_df = pd.read_csv(\"/kaggle/input/twitter-hw2/train_dataset.csv\")\n",
    "# test_df = pd.read_csv(\"/kaggle/input/twitter-hw2/test_dataset.csv\")\n",
    "# val_df = pd.read_csv(\"/kaggle/input/twitter-hw2/val_dataset.csv\")\n",
    "############ kaggle ############\n",
    "\n",
    "\n",
    "print(f\"{train_df.head(20)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
